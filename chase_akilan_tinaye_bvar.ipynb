{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian VAR Nowcasting Pipeline\n",
        "\n",
        "This notebook wraps the refactored Minnesota-prior BVAR workflow so you can run it interactively in Google Colab or any Jupyter environment. It expects two CSV filesâ€”`updated_dataset_goods.csv` and `updated_dataset_services.csv`â€”each containing a `date` column plus numeric predictor columns and a target column (`goods_target` or `services_target`).\n",
        "\n",
        "**Workflow overview**\n",
        "- Install dependencies (NumPy, pandas, SciPy, scikit-learn, matplotlib)\n",
        "- Define the BVAR helper functions (Minnesota prior, posterior sampler, rolling evaluation)\n",
        "- Point to the dataset locations (e.g., `/content/updated_dataset_goods.csv` in Colab after uploading or mounting Drive)\n",
        "- Run cross-validation across lag/Î» configurations, generate rolling predictions, compute MAE/RMSE/RÂ², and save results/plots\n",
        "\n",
        "> ðŸ’¡ Tip: In Colab, upload the CSVs via the file browser or mount Google Drive, then update the `goods_path` / `services_path` variables before executing the pipeline cell."
      ],
      "id": "94c609b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: install dependencies (uncomment if needed)\n",
        "# %pip install numpy pandas scipy scikit-learn matplotlib --quiet"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5e123f80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import itertools\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.linalg import inv\n",
        "from scipy.stats import invwishart\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
        "LOGGER = logging.getLogger(\"bvar_pipeline\")\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DatasetConfig:\n",
        "    name: str\n",
        "    path: Path\n",
        "    date_column: Optional[str] = \"date\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ModelConfig:\n",
        "    lags: int\n",
        "    lambda1: float = 0.3\n",
        "    lambda3: float = 1.0\n",
        "    include_const: bool = True\n",
        "    draws: int = 300\n",
        "    horizons: Tuple[int, ...] = (1, 2, 3)\n",
        "    train_fraction: float = 0.7\n",
        "    random_seed: Optional[int] = 2025\n",
        "\n",
        "\n",
        "def estimate_residual_variances(series: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Estimate residual variances via AR(1) regressions for each variable.\"\"\"\n",
        "    T, K = series.shape\n",
        "    variances = np.zeros(K)\n",
        "    eps = 1e-6\n",
        "    if T < 3:\n",
        "        variances.fill(1.0)\n",
        "        return variances\n",
        "    for idx in range(K):\n",
        "        y = series[1:, idx]\n",
        "        x = series[:-1, idx]\n",
        "        if len(x) < 2:\n",
        "            variances[idx] = max(np.var(y, ddof=1), eps)\n",
        "            continue\n",
        "        beta = np.linalg.lstsq(x[:, None], y, rcond=None)[0]\n",
        "        resid = y - x * beta\n",
        "        var = np.var(resid, ddof=1)\n",
        "        if not np.isfinite(var) or var <= 0:\n",
        "            var = np.var(series[:, idx], ddof=1)\n",
        "        variances[idx] = max(var, eps)\n",
        "    return variances\n",
        "\n",
        "\n",
        "def minnesota_prior(\n",
        "    history: np.ndarray,\n",
        "    p: int,\n",
        "    lambda1: float = 0.3,\n",
        "    lambda3: float = 1.0,\n",
        "    include_const: bool = True,\n",
        "    random_walk_prior: bool = True,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Construct Minnesota prior mean (B0) and covariance (V0) for the BVAR.\"\"\"\n",
        "    _, K = history.shape\n",
        "    offset = 1 if include_const else 0\n",
        "    M = K * p + offset\n",
        "    B0 = np.zeros((M, K))\n",
        "    if random_walk_prior and p >= 1:\n",
        "        for k in range(K):\n",
        "            B0[offset + k, k] = 1.0\n",
        "    sigma_sq = estimate_residual_variances(history)\n",
        "    V0 = np.zeros((M, M))\n",
        "    if include_const:\n",
        "        V0[0, 0] = 10.0\n",
        "    eps = 1e-6\n",
        "    for lag in range(1, p + 1):\n",
        "        for j in range(K):\n",
        "            row_idx = offset + (lag - 1) * K + j\n",
        "            for k in range(K):\n",
        "                if j == k:\n",
        "                    variance = (lambda1 ** 2) / (lag ** (2 * lambda3))\n",
        "                else:\n",
        "                    variance = (\n",
        "                        (lambda1 ** 2) * (sigma_sq[k] / max(sigma_sq[j], eps))\n",
        "                    ) / (lag ** (2 * lambda3))\n",
        "                V0[row_idx, row_idx] = max(variance, eps)\n",
        "    return B0, V0\n",
        "\n",
        "\n",
        "def build_var_matrices(\n",
        "    series: np.ndarray, p: int, include_const: bool = True\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    T, K = series.shape\n",
        "    if T <= p:\n",
        "        raise ValueError(\"Time series length must exceed the chosen lag order.\")\n",
        "    Y_rows, X_rows = [], []\n",
        "    for t in range(p, T):\n",
        "        lags = [series[t - lag] for lag in range(1, p + 1)]\n",
        "        reg = np.hstack(lags)\n",
        "        if include_const:\n",
        "            reg = np.hstack([1.0, reg])\n",
        "        X_rows.append(reg)\n",
        "        Y_rows.append(series[t])\n",
        "    return np.vstack(Y_rows), np.vstack(X_rows)\n",
        "\n",
        "\n",
        "def bvar_posterior(\n",
        "    Y: np.ndarray,\n",
        "    X: np.ndarray,\n",
        "    B0: np.ndarray,\n",
        "    V0: np.ndarray,\n",
        "    S0: Optional[np.ndarray] = None,\n",
        "    nu0: Optional[int] = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    N, K = Y.shape\n",
        "    if S0 is None:\n",
        "        S0 = np.eye(K)\n",
        "    if nu0 is None:\n",
        "        nu0 = K + 2\n",
        "    V0_inv = inv(V0)\n",
        "    XtX = X.T @ X\n",
        "    XtY = X.T @ Y\n",
        "    Vn = inv(V0_inv + XtX)\n",
        "    Bn = Vn @ (V0_inv @ B0 + XtY)\n",
        "    resid = Y - X @ Bn\n",
        "    term1 = resid.T @ resid\n",
        "    diff = Bn - B0\n",
        "    term2 = diff.T @ V0_inv @ diff\n",
        "    Sn = S0 + term1 + term2\n",
        "    nun = nu0 + N\n",
        "    return Bn, Vn, Sn, nun\n",
        "\n",
        "\n",
        "def sample_posterior(\n",
        "    Bn: np.ndarray,\n",
        "    Vn: np.ndarray,\n",
        "    Sn: np.ndarray,\n",
        "    nun: int,\n",
        "    draws: int,\n",
        "    random_state: Optional[int] = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    M, K = Bn.shape\n",
        "    B_samples = np.zeros((draws, M, K))\n",
        "    Sigma_samples = np.zeros((draws, K, K))\n",
        "    for i in range(draws):\n",
        "        Sigma_i = invwishart.rvs(df=nun, scale=Sn, random_state=rng)\n",
        "        Sigma_samples[i] = Sigma_i\n",
        "        cov = np.kron(Sigma_i, Vn)\n",
        "        mean = Bn.flatten(order=\"F\")\n",
        "        vec_sample = rng.multivariate_normal(mean=mean, cov=cov)\n",
        "        B_samples[i] = vec_sample.reshape(M, K, order=\"F\")\n",
        "    return B_samples, Sigma_samples\n",
        "\n",
        "\n",
        "def _stack_lags(history: np.ndarray, p: int) -> np.ndarray:\n",
        "    if history.shape[0] < p:\n",
        "        raise ValueError(\"Insufficient history to stack requested lags.\")\n",
        "    lags = [history[-lag] for lag in range(1, p + 1)]\n",
        "    return np.hstack(lags)\n",
        "\n",
        "\n",
        "def forecast_paths(\n",
        "    B_samples: np.ndarray,\n",
        "    Sigma_samples: np.ndarray,\n",
        "    history: np.ndarray,\n",
        "    p: int,\n",
        "    horizon: int,\n",
        "    include_const: bool = True,\n",
        "    random_state: Optional[int] = None,\n",
        ") -> np.ndarray:\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    draws, _, K = B_samples.shape\n",
        "    paths = np.zeros((draws, horizon, K))\n",
        "    for draw_idx in range(draws):\n",
        "        B = B_samples[draw_idx]\n",
        "        Sigma = Sigma_samples[draw_idx]\n",
        "        simulated = history.copy()\n",
        "        for step in range(horizon):\n",
        "            reg = _stack_lags(simulated, p)\n",
        "            if include_const:\n",
        "                reg = np.hstack([1.0, reg])\n",
        "            mean = reg @ B\n",
        "            shock = rng.multivariate_normal(np.zeros(K), Sigma)\n",
        "            y_next = mean + shock\n",
        "            paths[draw_idx, step] = y_next\n",
        "            simulated = np.vstack([simulated, y_next])\n",
        "    return paths\n",
        "\n",
        "\n",
        "def rolling_forecast(\n",
        "    series: np.ndarray,\n",
        "    var_names: Sequence[str],\n",
        "    config: ModelConfig,\n",
        ") -> pd.DataFrame:\n",
        "    T, K = series.shape\n",
        "    max_h = max(config.horizons)\n",
        "    min_train = max(int(T * config.train_fraction), config.lags + 5)\n",
        "    if T <= min_train + max_h:\n",
        "        LOGGER.warning(\"Not enough observations for rolling forecast (lags=%s)\", config.lags)\n",
        "        return pd.DataFrame()\n",
        "    records = []\n",
        "    base_rng = np.random.default_rng(config.random_seed)\n",
        "    for origin in range(min_train, T - max_h):\n",
        "        train = series[:origin]\n",
        "        try:\n",
        "            Y, X = build_var_matrices(train, config.lags, config.include_const)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        B0, V0 = minnesota_prior(\n",
        "            train,\n",
        "            p=config.lags,\n",
        "            lambda1=config.lambda1,\n",
        "            lambda3=config.lambda3,\n",
        "            include_const=config.include_const,\n",
        "        )\n",
        "        Bn, Vn, Sn, nun = bvar_posterior(Y, X, B0, V0)\n",
        "        B_samples, Sigma_samples = sample_posterior(\n",
        "            Bn,\n",
        "            Vn,\n",
        "            Sn,\n",
        "            nun,\n",
        "            draws=config.draws,\n",
        "            random_state=base_rng.integers(0, 2**32 - 1),\n",
        "        )\n",
        "        paths = forecast_paths(\n",
        "            B_samples,\n",
        "            Sigma_samples,\n",
        "            history=train,\n",
        "            p=config.lags,\n",
        "            horizon=max_h,\n",
        "            include_const=config.include_const,\n",
        "            random_state=base_rng.integers(0, 2**32 - 1),\n",
        "        )\n",
        "        mean_path = paths.mean(axis=0)\n",
        "        for horizon in config.horizons:\n",
        "            target_idx = origin + horizon\n",
        "            if target_idx >= T:\n",
        "                continue\n",
        "            prediction = mean_path[horizon - 1]\n",
        "            actual = series[target_idx]\n",
        "            for var_idx in range(K):\n",
        "                records.append(\n",
        "                    {\n",
        "                        \"origin_index\": origin,\n",
        "                        \"horizon\": horizon,\n",
        "                        \"variable_index\": var_idx,\n",
        "                        \"variable_name\": var_names[var_idx],\n",
        "                        \"prediction\": float(prediction[var_idx]),\n",
        "                        \"actual\": float(actual[var_idx]),\n",
        "                    }\n",
        "                )\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def compute_metrics(forecasts: pd.DataFrame) -> pd.DataFrame:\n",
        "    if forecasts.empty:\n",
        "        return pd.DataFrame()\n",
        "    metrics = []\n",
        "    grouped = forecasts.groupby([\"horizon\", \"variable_index\", \"variable_name\"])\n",
        "    for (horizon, var_idx, var_name), group in grouped:\n",
        "        y_true = group[\"actual\"].values\n",
        "        y_pred = group[\"prediction\"].values\n",
        "        if len(y_true) < 2:\n",
        "            continue\n",
        "        metrics.append(\n",
        "            {\n",
        "                \"horizon\": horizon,\n",
        "                \"variable_index\": var_idx,\n",
        "                \"variable_name\": var_name,\n",
        "                \"n_obs\": len(group),\n",
        "                \"MAE\": mean_absolute_error(y_true, y_pred),\n",
        "                \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "                \"R2\": r2_score(y_true, y_pred),\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "\n",
        "def evaluate_configs(\n",
        "    series: np.ndarray,\n",
        "    var_names: Sequence[str],\n",
        "    dataset_name: str,\n",
        "    lag_grid: Iterable[int],\n",
        "    lambda_grid: Iterable[float],\n",
        "    base_config: ModelConfig,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    all_metrics = []\n",
        "    best_score = np.inf\n",
        "    best_forecasts = pd.DataFrame()\n",
        "    for lags, lambda1 in itertools.product(lag_grid, lambda_grid):\n",
        "        config = ModelConfig(\n",
        "            lags=lags,\n",
        "            lambda1=lambda1,\n",
        "            lambda3=base_config.lambda3,\n",
        "            include_const=base_config.include_const,\n",
        "            draws=base_config.draws,\n",
        "            horizons=base_config.horizons,\n",
        "            train_fraction=base_config.train_fraction,\n",
        "            random_seed=base_config.random_seed,\n",
        "        )\n",
        "        LOGGER.info(\"Evaluating %s with lags=%d lambda1=%.3f\", dataset_name, lags, lambda1)\n",
        "        forecasts = rolling_forecast(series, var_names, config)\n",
        "        if forecasts.empty:\n",
        "            continue\n",
        "        metrics = compute_metrics(forecasts)\n",
        "        if metrics.empty:\n",
        "            continue\n",
        "        metrics = metrics.assign(dataset=dataset_name, lags=lags, lambda1=lambda1)\n",
        "        all_metrics.append(metrics)\n",
        "        mean_rmse = metrics[\"RMSE\"].mean()\n",
        "        if mean_rmse < best_score:\n",
        "            best_score = mean_rmse\n",
        "            best_forecasts = forecasts.assign(dataset=dataset_name, lags=lags, lambda1=lambda1)\n",
        "    metrics_df = pd.concat(all_metrics, ignore_index=True) if all_metrics else pd.DataFrame()\n",
        "    return metrics_df, best_forecasts\n",
        "\n",
        "\n",
        "def save_results(dataset_name: str, metrics: pd.DataFrame, forecasts: pd.DataFrame, results_dir: Path) -> None:\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "    metrics_path = results_dir / f\"{dataset_name}_metrics.csv\"\n",
        "    forecasts_path = results_dir / f\"{dataset_name}_forecasts.csv\"\n",
        "    if not metrics.empty:\n",
        "        metrics.to_csv(metrics_path, index=False)\n",
        "        LOGGER.info(\"Saved metrics to %s\", metrics_path)\n",
        "    if not forecasts.empty:\n",
        "        forecasts.to_csv(forecasts_path, index=False)\n",
        "        LOGGER.info(\"Saved forecasts to %s\", forecasts_path)\n",
        "\n",
        "\n",
        "def load_dataset(config: DatasetConfig) -> pd.DataFrame:\n",
        "    if not config.path.exists():\n",
        "        raise FileNotFoundError(f\"Dataset not found: {config.path}\")\n",
        "    df = pd.read_csv(config.path)\n",
        "    if config.date_column and config.date_column in df.columns:\n",
        "        df[config.date_column] = pd.to_datetime(df[config.date_column])\n",
        "        df = df.sort_values(config.date_column).set_index(config.date_column)\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    if numeric_df.empty:\n",
        "        raise ValueError(f\"No numeric columns found in dataset {config.path}\")\n",
        "    return numeric_df"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8156ca53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Update these paths for your environment (e.g., '/content/updated_dataset_goods.csv')\n",
        "goods_path = Path(\"updated_dataset_goods.csv\")\n",
        "services_path = Path(\"updated_dataset_services.csv\")\n",
        "\n",
        "lag_grid = (2, 3, 4)\n",
        "lambda_grid = (0.2, 0.3, 0.4)\n",
        "base_model = ModelConfig(\n",
        "    lags=2,\n",
        "    lambda1=0.3,\n",
        "    lambda3=1.0,\n",
        "    include_const=True,\n",
        "    draws=250,\n",
        "    horizons=(1, 2, 3),\n",
        "    train_fraction=0.7,\n",
        "    random_seed=2025,\n",
        ")\n",
        "results_dir = Path(\"results\")\n",
        "\n",
        "datasets = [\n",
        "    DatasetConfig(name=\"goods\", path=goods_path),\n",
        "    DatasetConfig(name=\"services\", path=services_path),\n",
        "]\n",
        "\n",
        "all_metrics = []\n",
        "all_forecasts = []\n",
        "for dataset in datasets:\n",
        "    LOGGER.info(\"Starting pipeline for dataset=%s\", dataset.name)\n",
        "    df = load_dataset(dataset)\n",
        "    series = df.to_numpy()\n",
        "    var_names = list(df.columns)\n",
        "    metrics_df, best_forecasts_df = evaluate_configs(\n",
        "        series=series,\n",
        "        var_names=var_names,\n",
        "        dataset_name=dataset.name,\n",
        "        lag_grid=lag_grid,\n",
        "        lambda_grid=lambda_grid,\n",
        "        base_config=base_model,\n",
        "    )\n",
        "    save_results(dataset.name, metrics_df, best_forecasts_df, results_dir)\n",
        "    all_metrics.append(metrics_df.assign(dataset=dataset.name))\n",
        "    all_forecasts.append(best_forecasts_df.assign(dataset=dataset.name))\n",
        "\n",
        "metrics_summary = pd.concat(all_metrics, ignore_index=True) if all_metrics else pd.DataFrame()\n",
        "forecasts_summary = pd.concat(all_forecasts, ignore_index=True) if all_forecasts else pd.DataFrame()\n",
        "metrics_summary.head()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6aa9e662"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: inspect summary tables\n",
        "if not metrics_summary.empty:\n",
        "    display(metrics_summary.sort_values([\"dataset\", \"horizon\", \"variable_index\"]))\n",
        "if not forecasts_summary.empty:\n",
        "    display(forecasts_summary.head())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3be72737"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Review the saved CSV outputs in the `results/` folder (`*_metrics.csv`, `*_forecasts.csv`).\n",
        "- Use the forecast tables to plot predicted vs actual values or export charts as needed.\n",
        "- Adjust `lag_grid`, `lambda_grid`, or model hyperparameters to explore alternative Minnesota prior settings."
      ],
      "id": "5a8b6b7c"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}