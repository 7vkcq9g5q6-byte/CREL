# -*- coding: utf-8 -*-
"""chase_akilan_tinaye_bvar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eKSCLLGyS7ZDJ1TQSuE8SKPZMvvgBUQX
"""





"""**Bayesian Vector Autoregression**

These are some good background papers to learn more about BVAR:

Bayesian Vector Autoregressions, Forecasting with Bayesian Vector Autoregressions: Five Years of Experience,Prior Selection for Vector Autoregressions (ECB)


Use the updated goods and services datasets (updated_dataset_goods.csv, updated_dataset_services.csv)

Build a Bayesian VAR for goods and another for services using 2–4 lags (vibecoding + AI is your friend here!)

You will probably use Python libraries pymc and statsmodels here

Use a standard Minnesota prior (start with lambda 1 = 0.3, and lambda 3 = 1.0)

Generate 1- to 3-month-ahead nowcasts and save predicted vs actual values.

Calculate RMSE, MAE, and R^2
"""

# build the bvar architecture
import numpy as np
from scipy.stats import invgamma, t, multivariate_normal #import normal distribution
# and other relevant parameters

def build_ar_dataset(y, p):
    # y: 1D array length N
    N = len(y)
    X = []
    Y = []
    for t in range(p, N):
        X.append(y[t-1 :t-p-1: -1])  # objects of the form indices
        Y.append(y[t])
    return np.array(Y), np.array(X)  # iterated outputs

def nig_posterior(X, y, beta0 = None, V0 = None, a0 = 1, b0 = 1):
    n, p = X.shape
    if beta0 is None:
        beta0 = np.zeros(p)
    if V0 is None:
        V0 = np.eye(p) * 1e6   # weak prior (scale by order 10^6 to limit the coefficients)
    V0_inv = np.linalg.inv(V0)

    XTX = X.T @ X # transpose matrix products
    XTY = X.T @ y # transpose matrix products
    Vn_inv = V0_inv + XTX
    Vn = np.linalg.inv(Vn_inv)
    betan = Vn @ (V0_inv @ beta0 + XTY)

    an = a0 + n/2
    # compute b_n using iterated updating
    term = y.T @ y + beta0.T @ V0_inv @ beta0 - betan.T @ Vn_inv @ betan
    bn = b0 + 0.5 * term

    return betan, Vn, an, bn

def sample_posterior(betan, Vn, an, bn, draws=1000):
    p = len(betan)
    sigma2_samples = invgamma.rvs(a=an, scale=bn, size=draws)
    beta_samples = np.zeros((draws, p))
    for i in range(draws):
        s2 = sigma2_samples[i]
        beta_samples[i] = multivariate_normal.rvs(mean=betan, cov=s2 * Vn)
    return beta_samples, sigma2_samples

def posterior_predictive(x_star, betan, Vn, an, bn, nsamples=1000):
    # returns samples from posterior predictive for x_star
    beta_samps, s2_samps = sample_posterior(betan, Vn, an, bn, draws=nsamples)
    preds = np.array([np.random.normal(loc=x_star @ b, scale=np.sqrt(s2))
                      for b, s2 in zip(beta_samps, s2_samps)])
    return preds

# Example usage
if __name__ == "__main__":
    # simulate AR(2): y_t = 0.6 y_{t-1} - 0.2 y_{t-2} + eps
    np.random.seed(1)
    N = 300
    phi = np.array([0.6, -0.2])
    sigma = 0.5
    y = np.zeros(N)
    for t in range(2, N):
        y[t] = phi @ y[t-1:t-3:-1] + np.random.normal(scale=sigma)

    p = 2
    Y, X = build_ar_dataset(y, p)
    beta0 = np.zeros(p)
    V0 = np.eye(p) * 10.0   # weak prior
    a0, b0 = 2.0, 1.0

    betan, Vn, an, bn = nig_posterior(X, Y, beta0, V0, a0, b0)
    print("Posterior mean beta:", betan)
    print("Posterior scale (b_n/a_n):", bn / an)

    # one-step ahead predictive from last observed window
    x_last = y[-1:-p-1:-1]  # [y_N, y_{N-1}, ...]
    preds = posterior_predictive(x_last, betan, Vn, an, bn, nsamples=2000)
    print("Predictive mean:", preds.mean(), "Predictive std:", preds.std())
    # credible interval
    print("95% CI:", np.percentile(preds, [2.5, 97.5]))

# an alternate version using minnesota prior
"""
Bayesian AR(p) with Minnesota prior (univariate) -- supports p in {2,3,4}.

Author: ChatGPT (GPT-5 Thinking mini)
"""

import numpy as np
from scipy.stats import invgamma, multivariate_normal
import matplotlib.pyplot as plt

# ----------------------------
# Utilities: build AR dataset
# ----------------------------
def build_ar_dataset(y, p):
    """
    Create X, Y for AR(p) regression.
    y: 1D array length N
    Returns: Y (n,), X (n, p) where row t corresponds to y_{t} = X_t @ beta + eps
    (we index t = p .. N-1)
    """
    N = len(y)
    if N <= p:
        raise ValueError("Series too short for chosen p")
    X = []
    Y = []
    for t in range(p, N):
        X.append(y[t-1:t-p-1:-1])  # [y_{t-1}, y_{t-2}, ..., y_{t-p}]
        Y.append(y[t])
    return np.asarray(Y), np.asarray(X)

# ----------------------------------------
# Minnesota prior construction (univariate)
# ----------------------------------------
def construct_minnesota_prior(p, sigma_hat, lambda1=0.2, lambda2=1.0, lambda3=1.0,
                              random_walk_prior=False):
    """
    Construct prior mean beta0 and prior covariance V0 (p x p) for a univariate AR(p)
    under a Minnesota style prior.

    Args:
        p: number of lags
        sigma_hat: estimated residual std (scalar) used to scale prior variances
        lambda1: overall tightness (typical default 0.1-0.5)
        lambda2: own-vs-others shrink (kept for API; in univariate it doesn't change)
        lambda3: lag decay exponent (typical =1)
        random_walk_prior: if True, set prior mean for first lag = 1 (random-walk)
    Returns:
        beta0: (p,) prior mean
        V0: (p,p) prior covariance matrix (not scaled by sigma^2, see conjugacy form)
             We will return V0 such that prior is beta | sigma^2 ~ N(beta0, sigma^2 * V0)
    """
    beta0 = np.zeros(p)
    if random_walk_prior:
        # Put prior mean 1 on first own lag (common in Minnesota for levels)
        beta0[0] = 1.0

    # Diagonal prior variance (Minnesota). For univariate AR, sigma ratio = 1.
    var_diag = np.zeros(p)
    for lag in range(1, p+1):
        # typical Minnesota formula (univariate simplification):
        # Var(beta_l) = (lambda1^2 * sigma_hat^2) / lag^{2*lambda3}
        var_diag[lag-1] = (lambda1 ** 2) * (sigma_hat ** 2) / (lag ** (2.0 * lambda3))

    V0 = np.diag(var_diag)  # V0 is the covariance matrix multiplier of sigma^2
    return beta0, V0

# ----------------------------------------------------
# Conjugate NIG posterior functions (beta | sigma^2 ~ NIG)
# ----------------------------------------------------
def nig_posterior(X, y, beta0, V0, a0=1.0, b0=1.0):
    """
    Compute posterior parameters for Normal-Inverse-Gamma conjugate model:
      y ~ N(X beta, sigma^2 I)
      beta | sigma^2 ~ N(beta0, sigma^2 V0)
      sigma^2 ~ Inv-Gamma(a0, b0)

    Returns:
      betan (p,), Vn (p,p), an (scalar), bn (scalar)
    where posterior is beta | sigma^2, data ~ N(betan, sigma^2 Vn) and
          sigma^2 ~ Inv-Gamma(an, bn)
    """
    n, p = X.shape
    V0_inv = np.linalg.inv(V0)
    XtX = X.T @ X
    Xty = X.T @ y

    Vn_inv = V0_inv + XtX
    Vn = np.linalg.inv(Vn_inv)
    betan = Vn @ (V0_inv @ beta0 + Xty)

    an = a0 + n / 2.0
    # compute bn carefully to avoid numeric issues
    yTy = y.T @ y
    term = yTy + beta0.T @ V0_inv @ beta0 - betan.T @ Vn_inv @ betan
    bn = b0 + 0.5 * term

    return betan, Vn, an, bn

# -----------------------------------------
# Posterior sampling and posterior predictive
# -----------------------------------------
def sample_posterior(betan, Vn, an, bn, draws=2000, random_state=None):
    rng = np.random.default_rng(random_state)
    p = len(betan)
    sigma2_samples = invgamma.rvs(a=an, scale=bn, size=draws, random_state=rng)
    beta_samples = np.zeros((draws, p))
    for i in range(draws):
        s2 = sigma2_samples[i]
        beta_samples[i] = rng.multivariate_normal(mean=betan, cov=s2 * Vn)
    return beta_samples, sigma2_samples

def posterior_predictive(x_star, betan, Vn, an, bn, nsamples=2000, random_state=None):
    """
    Draw posterior predictive samples for scalar x_star (1D array shape (p,))
    Returns array of length nsamples
    """
    beta_samps, s2_samps = sample_posterior(betan, Vn, an, bn, draws=nsamples, random_state=random_state)
    preds = np.array([rng_val for rng_val in ( (x_star @ b) + np.sqrt(s2)*np.random.normal()
                                              for b, s2 in zip(beta_samps, s2_samps) )])
    # The above uses np.random.normal with default RNG; to keep reproducible,
    # we'll vectorize differently:
    rng = np.random.default_rng(random_state)
    eps = rng.normal(size=nsamples) * np.sqrt(s2_samps)
    preds = np.einsum('ij,i->i', beta_samps, x_star) + eps
    return preds

# ------------------------------
# Example / Demonstration script
# ------------------------------
def demo(simulate=True, p=3, N=400, random_seed=123,
         lambda1=0.2, lambda3=1.0, random_walk_prior=False):
    """
    Demonstration pipeline:
      - simulate or load data
      - build AR dataset for p
      - estimate sigma_hat from OLS residuals (or small AR)
      - construct Minnesota prior
      - compute NIG posterior
      - posterior predictive for last observed window
    """
    rng = np.random.default_rng(random_seed)

    if simulate:
        # simulate an AR(3) truth for demo (could be any p_truth)
        phi_true = np.array([0.6, -0.25, 0.1])  # length 3
        sigma_true = 0.4
        y = np.zeros(N)
        # burn-in
        for t in range(3, N):
            y[t] = phi_true @ y[t-1:t-4:-1] + rng.normal(scale=sigma_true)
        ts = y
    else:
        raise NotImplementedError("Load your time series here")

    # If requested p differs from truth, we still fit AR(p)
    Y, X = build_ar_dataset(ts, p)

    # initial OLS estimate to get residual std for Minnesota scaling
    # (OLS may be singular if X'X not full rank, but for moderate N it's fine)
    beta_ols = np.linalg.lstsq(X, Y, rcond=None)[0]
    resid = Y - X @ beta_ols
    sigma_hat = np.sqrt(max(np.var(resid, ddof=1), 1e-8))

    print(f"Using p={p}. OLS residual std estimate sigma_hat = {sigma_hat:.4f}")

    # Minnesota prior
    beta0, V0 = construct_minnesota_prior(p, sigma_hat,
                                          lambda1=lambda1, lambda3=lambda3,
                                          random_walk_prior=random_walk_prior)
    # Weak prior on sigma2 (tunable)
    a0, b0 = 2.0, 0.5

    betan, Vn, an, bn = nig_posterior(X, Y, beta0, V0, a0=a0, b0=b0)

    print("Posterior mean (betan):", betan)
    print("Posterior covariance times sigma^2 (Vn diag):", np.diag(Vn))
    print("Posterior a_n, b_n:", an, bn)
    print("Posterior expected sigma^2 (bn/(an-1)):", bn / (an - 1.0))

    # one-step ahead predictive using final observed window
    x_last = ts[-1:-p-1:-1]  # last p observations as features
    preds = posterior_predictive(x_last, betan, Vn, an, bn, nsamples=4000, random_state=random_seed)

    print("Predictive mean:", preds.mean(), "Predictive std:", preds.std())
    print("95% predictive interval:", np.percentile(preds, [2.5, 97.5]))

    # Simple diagnostic plot
    plt.figure(figsize=(10,4))
    plt.hist(preds, bins=40, density=True)
    plt.axvline(preds.mean(), color='C1', linestyle='--', label='pred mean')
    plt.title("Posterior predictive histogram (one-step ahead)")
    plt.legend()
    plt.show()

    # Return objects for further analysis
    return {
        'ts': ts,
        'X': X, 'Y': Y,
        'beta0': beta0, 'V0': V0,
        'betan': betan, 'Vn': Vn, 'an': an, 'bn': bn,
        'preds': preds
    }

# ------------------------------
# Run demo when executed
# ------------------------------
if __name__ == "__main__":
    # allow user to choose p = 2..4
    result = demo(simulate=True, p=3, N=600, random_seed=42,
                  lambda1=0.15, lambda3=1.0, random_walk_prior=False)

# testing on specific dataset

# get the errors

# Tinaye - trying to get the BVAR to run with datasets

#VIBE CODE WITH CHAT GPT

import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal

############################
# 1. Load and prep the data
############################

df = pd.read_csv("dataset_goods.csv")

# Keep numeric endogenous variables (drop date)
# Assumes all columns after 'date' are endogenous series
endog_cols = df.columns[1:]  # skip 'date'
Y_all = df[endog_cols].to_numpy()  # shape (T, K)

############################
# 2. Helper: build VAR dataset
############################

def build_var_dataset(Y_all, p, add_const=True):
    """
    Y_all: array shape (T, K)
    p: number of lags
    add_const: whether to include intercept column of 1s
    returns:
        Y_mat: (T-p, K)   current values
        X_mat: (T-p, K*p [+1]) lagged regressors
        last_lags: vector of the most recent p lags (for forecasting)
                   shape (K*p [+1],) if add_const else (K*p,)
    """
    T, K = Y_all.shape
    Y_list = []
    X_list = []

    for t in range(p, T):
        # stack lags [y_{t-1}, y_{t-2}, ..., y_{t-p}]
        lags = [Y_all[t - lag] for lag in range(1, p+1)]
        x_t = np.hstack(lags)  # shape (K*p,)
        if add_const:
            x_t = np.hstack([1.0, x_t])  # prepend intercept
        X_list.append(x_t)

        Y_list.append(Y_all[t])  # shape (K,)

    Y_mat = np.vstack(Y_list)        # (T-p, K)
    X_mat = np.vstack(X_list)        # (T-p, K*p+1) if const else (T-p, K*p)

    # also build last_lags for one-step-ahead forecast
    recent_lags = [Y_all[-lag] for lag in range(1, p+1)]
    x_last = np.hstack(recent_lags)
    if add_const:
        x_last = np.hstack([1.0, x_last])

    return Y_mat, X_mat, x_last

############################
# 3. Posterior update (Matrix-Normal / Inverse-Wishart)
############################

def bvar_posterior(Y_mat, X_mat,
                   B0=None, V0=None,
                   S0=None, nu0=None):
    """
    Y_mat: (N, K)
    X_mat: (N, M)  where M = K*p + (1 if intercept)
    Prior:
      B | Sigma ~ MatrixNormal(B0, V0, Sigma)
      Sigma ~ InverseWishart(S0, nu0)
    Returns posterior hyperparameters:
      Bn, Vn, Sn, nun
    """
    N, K = Y_mat.shape
    M = X_mat.shape[1]

    # default weak-ish priors if not provided
    if B0 is None:
        B0 = np.zeros((M, K))
    if V0 is None:
        # large diagonal -> weak prior
        V0 = np.eye(M) * 10.0
    if S0 is None:
        S0 = np.eye(K)
    if nu0 is None:
        nu0 = K + 2  # just > K so IW is proper but not too tight

    V0_inv = inv(V0)

    XtX = X_mat.T @ X_mat   # (M, M)
    XtY = X_mat.T @ Y_mat   # (M, K)

    Vn = inv(V0_inv + XtX)  # (M, M)
    Bn = Vn @ (V0_inv @ B0 + XtY)  # (M, K)

    # Compute Sn (scale matrix for Sigma posterior)
    # Sn = S0 + (Y - X Bn)'(Y - X Bn) + (Bn - B0)' V0_inv (Bn - B0)
    resid = Y_mat - X_mat @ Bn            # (N, K)
    term1 = resid.T @ resid               # (K, K)

    diffB = (Bn - B0)                     # (M, K)
    term2 = diffB.T @ V0_inv @ diffB      # (K, K)

    Sn = S0 + term1 + term2
    nun = nu0 + N

    return Bn, Vn, Sn, nun

############################
# 4. Sampling from the posterior
############################

def sample_bvar_posterior(Bn, Vn, Sn, nun, draws=1000):
    """
    Draws (B, Sigma) pairs from the posterior:
      Sigma ~ InvWishart(Sn, nun)
      B | Sigma ~ MatrixNormal(Bn, Vn, Sigma)
    Returns:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
    """
    M, K = Bn.shape
    B_samps = np.zeros((draws, M, K))
    Sigma_samps = np.zeros((draws, K, K))

    for i in range(draws):
        # sample Sigma ~ InvWishart
        Sigma_i = invwishart.rvs(df=nun, scale=Sn)  # (K, K)
        Sigma_samps[i] = Sigma_i

        # sample B | Sigma
        # vec(B) ~ N(vec(Bn), kron(Sigma, Vn))
        # We can draw using multivariate_normal on the vectorized form:
        cov = np.kron(Sigma_i, Vn)  # (M*K, M*K)
        mean = Bn.flatten(order='F')  # vec in column-major (match kron convention)
        B_vec_i = multivariate_normal.rvs(mean=mean, cov=cov)
        B_i = B_vec_i.reshape(M, K, order='F')
        B_samps[i] = B_i

    return B_samps, Sigma_samps

############################
# 5. One-step-ahead predictive draws
############################

def forecast_one_step(B_samps, Sigma_samps, x_last):
    """
    Produce predictive draws for y_{T+1} given last lags x_last.
    Inputs:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
      x_last: shape (M,) = [1, last_lags...] if intercept was used
    Returns:
      y_pred_samps: (draws, K)
    """
    draws, M, K = B_samps.shape
    y_pred_samps = np.zeros((draws, K))
    for i in range(draws):
        B_i = B_samps[i]           # (M, K)
        Sigma_i = Sigma_samps[i]   # (K, K)

        mean_i = x_last @ B_i      # (K,)
        shock_i = multivariate_normal.rvs(mean=np.zeros(K), cov=Sigma_i)
        y_pred_samps[i] = mean_i + shock_i
    return y_pred_samps

############################
# 6. Put it all together
############################

p = 2  # VAR lag order
Y_mat, X_mat, x_last = build_var_dataset(Y_all, p, add_const=True)

# Get posterior hyperparameters
Bn, Vn, Sn, nun = bvar_posterior(Y_mat, X_mat)

# Draw from posterior
B_samps, Sigma_samps = sample_bvar_posterior(Bn, Vn, Sn, nun, draws=2000)

# Posterior predictive for next period
y_pred_samps = forecast_one_step(B_samps, Sigma_samps, x_last)
# y_pred_samps shape: (2000, K)

# Focus on the FIRST variable (column 0 = tn_air_traffic_value_short)
first_var_preds = y_pred_samps[:, 0]

print("Forecast mean for first variable:", first_var_preds.mean())
print("Forecast std for first variable:", first_var_preds.std())
print("95% credible interval for first variable:",
      np.percentile(first_var_preds, [2.5, 97.5]))

# modified iteration of code (Akilan) for dealing with the two datasets

import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal
import matplotlib.pyplot as plt

############################
# 1. Load and prep the data
############################

df = pd.read_csv("dataset_goods.csv") # Keep numeric endogenous variables (drop date) # Assumes all columns after 'date' are endogenous series
endog_cols = df.columns[1:] # skip 'date'
Y_all = df[endog_cols].to_numpy() # shape (T, K)
############################
# 2. Helper: build VAR dataset
############################

def build_var_dataset(Y_all, p, add_const=True):
    """
    Y_all: array shape (T, K)
    p: number of lags
    add_const: whether to include intercept column of 1s
    returns:
        Y_mat: (T-p, K)   current values
        X_mat: (T-p, K*p [+1]) lagged regressors
        last_lags: vector of the most recent p lags (for forecasting)
                   shape (K*p [+1],) if add_const else (K*p,)
    """
    T, K = Y_all.shape
    Y_list = []
    X_list = []

    for t in range(p, T):
        # stack lags [y_{t-1}, y_{t-2}, ..., y_{t-p}]
        lags = [Y_all[t - lag] for lag in range(1, p+1)]
        x_t = np.hstack(lags)  # shape (K*p,)
        if add_const:
            x_t = np.hstack([1.0, x_t])  # prepend intercept
        X_list.append(x_t)

        Y_list.append(Y_all[t])  # shape (K,)

    Y_mat = np.vstack(Y_list)        # (T-p, K)
    X_mat = np.vstack(X_list)        # (T-p, K*p+1) if const else (T-p, K*p)

    # also build last_lags for one-step-ahead forecast
    recent_lags = [Y_all[-lag] for lag in range(1, p+1)]
    x_last = np.hstack(recent_lags)
    if add_const:
        x_last = np.hstack([1.0, x_last])

    return Y_mat, X_mat, x_last

############################
# 3. Posterior update (Matrix-Normal / Inverse-Wishart)
############################

def bvar_posterior(Y_mat, X_mat,
                   B0=None, V0=None,
                   S0=None, nu0=None):
    """
    Y_mat: (N, K)
    X_mat: (N, M)  where M = K*p + (1 if intercept)
    Prior:
      B | Sigma ~ MatrixNormal(B0, V0, Sigma)
      Sigma ~ InverseWishart(S0, nu0)
    Returns posterior hyperparameters:
      Bn, Vn, Sn, nun
    """
    N, K = Y_mat.shape
    M = X_mat.shape[1]

    # default weak-ish priors if not provided
    if B0 is None:
        B0 = np.zeros((M, K))
    if V0 is None:
        # large diagonal -> weak prior
        V0 = np.eye(M) * 10.0
    if S0 is None:
        S0 = np.eye(K)
    if nu0 is None:
        nu0 = K + 2  # just > K so IW is proper but not too tight

    V0_inv = inv(V0)

    XtX = X_mat.T @ X_mat   # (M, M)
    XtY = X_mat.T @ Y_mat   # (M, K)

    Vn = inv(V0_inv + XtX)  # (M, M)
    Bn = Vn @ (V0_inv @ B0 + XtY)  # (M, K)

    # Compute Sn (scale matrix for Sigma posterior)
    # Sn = S0 + (Y - X Bn)'(Y - X Bn) + (Bn - B0)' V0_inv (Bn - B0)
    resid = Y_mat - X_mat @ Bn            # (N, K)
    term1 = resid.T @ resid               # (K, K)

    diffB = (Bn - B0)                     # (M, K)
    term2 = diffB.T @ V0_inv @ diffB      # (K, K)

    Sn = S0 + term1 + term2
    nun = nu0 + N

    return Bn, Vn, Sn, nun

############################
# 4. Sampling from the posterior
############################

def sample_bvar_posterior(Bn, Vn, Sn, nun, draws=1000):
    """
    Draws (B, Sigma) pairs from the posterior:
      Sigma ~ InvWishart(Sn, nun)
      B | Sigma ~ MatrixNormal(Bn, Vn, Sigma)
    Returns:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
    """
    M, K = Bn.shape
    B_samps = np.zeros((draws, M, K))
    Sigma_samps = np.zeros((draws, K, K))

    for i in range(draws):
        # sample Sigma ~ InvWishart
        Sigma_i = invwishart.rvs(df=nun, scale=Sn)  # (K, K)
        Sigma_samps[i] = Sigma_i

        # sample B | Sigma
        # vec(B) ~ N(vec(Bn), kron(Sigma, Vn))
        # We can draw using multivariate_normal on the vectorized form:
        cov = np.kron(Sigma_i, Vn)  # (M*K, M*K)
        mean = Bn.flatten(order='F')  # vec in column-major (match kron convention)
        B_vec_i = multivariate_normal.rvs(mean=mean, cov=cov)
        B_i = B_vec_i.reshape(M, K, order='F')
        B_samps[i] = B_i

    return B_samps, Sigma_samps

############################
# 5. One-step-ahead predictive draws
############################

def forecast_one_step(B_samps, Sigma_samps, x_last):
    """
    Produce predictive draws for y_{T+1} given last lags x_last.
    Inputs:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
      x_last: shape (M,) = [1, last_lags...] if intercept was used
    Returns:
      y_pred_samps: (draws, K)
    """
    draws, M, K = B_samps.shape
    y_pred_samps = np.zeros((draws, K))
    for i in range(draws):
        B_i = B_samps[i]           # (M, K)
        Sigma_i = Sigma_samps[i]   # (K, K)

        mean_i = x_last @ B_i      # (K,)
        shock_i = multivariate_normal.rvs(mean=np.zeros(K), cov=Sigma_i)
        y_pred_samps[i] = mean_i + shock_i
    return y_pred_samps

############################
# 6. Put it all together
############################

p = 2  # VAR lag order
Y_mat, X_mat, x_last = build_var_dataset(Y_all, p, add_const=True)

# Get posterior hyperparameters
Bn, Vn, Sn, nun = bvar_posterior(Y_mat, X_mat)

# Draw from posterior
B_samps, Sigma_samps = sample_bvar_posterior(Bn, Vn, Sn, nun, draws=2000)
r

# visualization

# --- Plot forecast distributions ---
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    ax = axes[i]
    ax.hist(y_pred_samps[:, i], bins=30, color="skyblue", edgecolor="k", alpha=0.7)
    ax.axvline(means[i], color="red", lw=2, label="Mean forecast")
    ax.axvline(ci_lower[i], color="gray", ls="--")
    ax.axvline(ci_upper[i], color="gray", ls="--")
    ax.set_title(f"Posterior Predictive for {var}")
    ax.legend()
plt.tight_layout()
plt.show()

# --- Fitted vs Actual values (in-sample fit) ---
Y_fit = X_mat @ Bn
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    axes[i].plot(Y_mat[:, i], label="Actual", lw=2)
    axes[i].plot(Y_fit[:, i], label="Fitted", lw=2, linestyle="--")
    axes[i].set_title(f"In-sample Fit: {var}")
    axes[i].legend()
plt.tight_layout()
plt.show()

############################
# 8. Impulse Response Function (IRF) — basic version
############################

def impulse_response(B_samps, Sigma_samps, p, steps=10):
    draws, M, K = B_samps.shape
    # Take posterior mean parameters
    B_mean = B_samps.mean(axis=0)
    Sigma_mean = Sigma_samps.mean(axis=0)
    # reshape B to companion form
    A = B_mean[1:, :].T  # remove intercept, shape (K, K*p)
    comp = np.zeros((K*p, K*p))
    comp[:K, :] = A
    comp[K:, :-K] = np.eye(K*(p-1))
    # IRFs
    irfs = np.zeros((steps, K, K))
    irfs[0] = np.eye(K)
    for h in range(1, steps):
        irfs[h] = comp[:K, :K] @ irfs[h-1]
    # scale by Cholesky of Sigma
    P = np.linalg.cholesky(Sigma_mean)
    irfs = np.einsum("hij,jk->hik", irfs, P)
    return irfs

irfs = impulse_response(B_samps, Sigma_samps, p, steps=12)

# Plot IRFs
K = len(endog_cols)
fig, axes = plt.subplots(K, K, figsize=(3*K, 3*K))
for i in range(K):
    for j in range(K):
        axes[i,j].plot(irfs[:, i, j])
        axes[i,j].axhline(0, color="black", lw=1)
        axes[i,j].set_title(f"Response of {endog_cols[i]} to shock in {endog_cols[j]}")
plt.tight_layout()
plt.show()

# MINNESOTA version

# modified iteration of code (Akilan) for dealing with the two datasets

import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal
import matplotlib.pyplot as plt

############################
# 1. Load and prep the data
############################

df = pd.read_csv("dataset_goods.csv") # Keep numeric endogenous variables (drop date) # Assumes all columns after 'date' are endogenous series
endog_cols = df.columns[1:] # skip 'date'
Y_all = df[endog_cols].to_numpy() # shape (T, K)
############################
# 2. Helper: build VAR dataset
############################

def build_var_dataset(Y_all, p, add_const=True):
    """
    Y_all: array shape (T, K)
    p: number of lags
    add_const: whether to include intercept column of 1s
    returns:
        Y_mat: (T-p, K)   current values
        X_mat: (T-p, K*p [+1]) lagged regressors
        last_lags: vector of the most recent p lags (for forecasting)
                   shape (K*p [+1],) if add_const else (K*p,)
    """
    T, K = Y_all.shape
    Y_list = []
    X_list = []

    for t in range(p, T):
        # stack lags [y_{t-1}, y_{t-2}, ..., y_{t-p}]
        lags = [Y_all[t - lag] for lag in range(1, p+1)]
        x_t = np.hstack(lags)  # shape (K*p,)
        if add_const:
            x_t = np.hstack([1.0, x_t])  # prepend intercept
        X_list.append(x_t)

        Y_list.append(Y_all[t])  # shape (K,)

    Y_mat = np.vstack(Y_list)        # (T-p, K)
    X_mat = np.vstack(X_list)        # (T-p, K*p+1) if const else (T-p, K*p)

    # also build last_lags for one-step-ahead forecast
    recent_lags = [Y_all[-lag] for lag in range(1, p+1)]
    x_last = np.hstack(recent_lags)
    if add_const:
        x_last = np.hstack([1.0, x_last])

    return Y_mat, X_mat, x_last

############################
# 3. Posterior update (Matrix-Normal / Inverse-Wishart)
############################

def bvar_posterior(Y_mat, X_mat, p, lambda_1=0.3, lambda_3=1.0):
    """
    Minnesota prior consistent with X_mat ordering from build_var_dataset:
      [y_{t-1,1},...,y_{t-1,K}, y_{t-2,1},...,y_{t-p,K}]
    """
    N, K = Y_mat.shape
    M = X_mat.shape[1]

    # --- Step 1: estimate AR(1) residual variances ---
    sigma_sq = np.zeros(K)
    for i in range(K):
        y_i = Y_mat[1:, i]
        x_i = Y_mat[:-1, i]
        beta_i = np.linalg.lstsq(x_i[:, None], y_i, rcond=None)[0]
        resid_i = y_i - x_i * beta_i
        sigma_sq[i] = np.var(resid_i, ddof=1)

    # --- Step 2: prior mean ---
    B0 = np.zeros((M, K))
    for i in range(K):
        # own first lag coefficient = 1
        idx = 1 + i  # intercept = 0, then first lag block starts at 1
        B0[idx, i] = 1.0

    # --- Step 3: prior variance matrix ---
    V0 = np.zeros((M, M))
    V0[0, 0] = 10.0  # intercept prior variance (loose)

    # fill lag coefficients
    for lag in range(1, p + 1):
        for j in range(K):  # regressor variable
            idx = 1 + (lag - 1) * K + j  # linear index for (lag, variable)
            for i in range(K):  # equation (dependent variable)
                if i == j:
                    var_ij = (lambda_1 ** 2) / (lag ** 2)
                else:
                    var_ij = (lambda_1 ** 2) * (sigma_sq[i] / sigma_sq[j]) / ((lambda_3 ** 2) * (lag ** 2))
                # same variance across equations for coefficient on this regressor
                V0[idx, idx] = var_ij

    # --- Step 4: prior on Sigma ---
    S0 = np.eye(K)
    nu0 = K + 2

    # --- Step 5: posterior update ---
    V0_inv = inv(V0)
    XtX = X_mat.T @ X_mat
    XtY = X_mat.T @ Y_mat
    Vn = inv(V0_inv + XtX)
    Bn = Vn @ (V0_inv @ B0 + XtY)

    resid = Y_mat - X_mat @ Bn
    Sn = S0 + resid.T @ resid
    nun = nu0 + N

    return Bn, Vn, Sn, nun

############################
# 4. Sampling from the posterior
############################

def sample_bvar_posterior(Bn, Vn, Sn, nun, draws=1000):
    """
    Draws (B, Sigma) pairs from the posterior:
      Sigma ~ InvWishart(Sn, nun)
      B | Sigma ~ MatrixNormal(Bn, Vn, Sigma)
    Returns:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
    """
    M, K = Bn.shape
    B_samps = np.zeros((draws, M, K))
    Sigma_samps = np.zeros((draws, K, K))

    for i in range(draws):
        # sample Sigma ~ InvWishart
        Sigma_i = invwishart.rvs(df=nun, scale=Sn)  # (K, K)
        Sigma_samps[i] = Sigma_i

        # sample B | Sigma
        # vec(B) ~ N(vec(Bn), kron(Sigma, Vn))
        # We can draw using multivariate_normal on the vectorized form:
        cov = np.kron(Sigma_i, Vn)  # (M*K, M*K)
        mean = Bn.flatten(order='F')  # vec in column-major (match kron convention)
        B_vec_i = multivariate_normal.rvs(mean=mean, cov=cov)
        B_i = B_vec_i.reshape(M, K, order='F')
        B_samps[i] = B_i

    return B_samps, Sigma_samps

############################
# 5. One-step-ahead predictive draws
############################

def forecast_one_step(B_samps, Sigma_samps, x_last):
    """
    Produce predictive draws for y_{T+1} given last lags x_last.
    Inputs:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
      x_last: shape (M,) = [1, last_lags...] if intercept was used
    Returns:
      y_pred_samps: (draws, K)
    """
    draws, M, K = B_samps.shape
    y_pred_samps = np.zeros((draws, K))
    for i in range(draws):
        B_i = B_samps[i]           # (M, K)
        Sigma_i = Sigma_samps[i]   # (K, K)

        mean_i = x_last @ B_i      # (K,)
        shock_i = multivariate_normal.rvs(mean=np.zeros(K), cov=Sigma_i)
        y_pred_samps[i] = mean_i + shock_i
    return y_pred_samps

############################
# 6. Put it all together
############################

p = 2  # VAR lag order
Y_mat, X_mat, x_last = build_var_dataset(Y_all, p, add_const=True)

# Get posterior hyperparameters
Bn, Vn, Sn, nun = bvar_posterior(Y_mat, X_mat, p, lambda_1=0.3, lambda_3=1.0)

# Draw from posterior
B_samps, Sigma_samps = sample_bvar_posterior(Bn, Vn, Sn, nun, draws=2000)

# Posterior predictive for next period
y_pred_samps = forecast_one_step(B_samps, Sigma_samps, x_last)
# y_pred_samps shape: (2000, K)

# Focus on the FIRST variable (column 0 = tn_air_traffic_value_short)
first_var_preds = y_pred_samps[:, 0]

print("Forecast mean for first variable:", first_var_preds.mean())
print("Forecast std for first variable:", first_var_preds.std())
print("95% credible interval for first variable:",
      np.percentile(first_var_preds, [2.5, 97.5]))

# --- Posterior predictive summary ---
means = y_pred_samps.mean(axis=0)
stds = y_pred_samps.std(axis=0)
ci_lower = np.percentile(y_pred_samps, 2.5, axis=0)
ci_upper = np.percentile(y_pred_samps, 97.5, axis=0)

summary_df = pd.DataFrame({
    "Variable": endog_cols,
    "Mean Forecast": means,
    "Std": stds,
    "95% Lower": ci_lower,
    "95% Upper": ci_upper
})
print("\n=== Posterior Forecast Summary ===")
print(summary_df)

# visualization

# --- Plot forecast distributions ---
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    ax = axes[i]
    ax.hist(y_pred_samps[:, i], bins=30, color="skyblue", edgecolor="k", alpha=0.7)
    ax.axvline(means[i], color="red", lw=2, label="Mean forecast")
    ax.axvline(ci_lower[i], color="gray", ls="--")
    ax.axvline(ci_upper[i], color="gray", ls="--")
    ax.set_title(f"Posterior Predictive for {var}")
    ax.legend()
plt.tight_layout()
plt.show()

# --- Fitted vs Actual values (in-sample fit) ---
Y_fit = X_mat @ Bn
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    axes[i].plot(Y_mat[:, i], label="Actual", lw=2)
    axes[i].plot(Y_fit[:, i], label="Fitted", lw=2, linestyle="--")
    axes[i].set_title(f"In-sample Fit: {var}")
    axes[i].legend()
plt.tight_layout()
plt.show()

############################
# 8. Impulse Response Function (IRF) — basic version
############################

def impulse_response(B_samps, Sigma_samps, p, steps=10):
    draws, M, K = B_samps.shape
    # Take posterior mean parameters
    B_mean = B_samps.mean(axis=0)
    Sigma_mean = Sigma_samps.mean(axis=0)
    # reshape B to companion form
    A = B_mean[1:, :].T  # remove intercept, shape (K, K*p)
    comp = np.zeros((K*p, K*p))
    comp[:K, :] = A
    comp[K:, :-K] = np.eye(K*(p-1))
    # IRFs
    irfs = np.zeros((steps, K, K))
    irfs[0] = np.eye(K)
    for h in range(1, steps):
        irfs[h] = comp[:K, :K] @ irfs[h-1]
    # scale by Cholesky of Sigma
    P = np.linalg.cholesky(Sigma_mean)
    irfs = np.einsum("hij,jk->hik", irfs, P)
    return irfs

irfs = impulse_response(B_samps, Sigma_samps, p, steps=12)

# Plot IRFs
K = len(endog_cols)
fig, axes = plt.subplots(K, K, figsize=(3*K, 3*K))
for i in range(K):
    for j in range(K):
        axes[i,j].plot(irfs[:, i, j])
        axes[i,j].axhline(0, color="black", lw=1)
        axes[i,j].set_title(f"Response of {endog_cols[i]} to shock in {endog_cols[j]}")
plt.tight_layout()
plt.show()

# test the minnesota with different parameters (lambda_1 = 0.4)
# MINNESOTA version

# modified iteration of code (Akilan) for dealing with the two datasets

import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal
import matplotlib.pyplot as plt

############################
# 1. Load and prep the data
############################

df = pd.read_csv("dataset_goods.csv") # Keep numeric endogenous variables (drop date) # Assumes all columns after 'date' are endogenous series
endog_cols = df.columns[1:] # skip 'date'
Y_all = df[endog_cols].to_numpy() # shape (T, K)
############################
# 2. Helper: build VAR dataset
############################

def build_var_dataset(Y_all, p, add_const=True):
    """
    Y_all: array shape (T, K)
    p: number of lags
    add_const: whether to include intercept column of 1s
    returns:
        Y_mat: (T-p, K)   current values
        X_mat: (T-p, K*p [+1]) lagged regressors
        last_lags: vector of the most recent p lags (for forecasting)
                   shape (K*p [+1],) if add_const else (K*p,)
    """
    T, K = Y_all.shape
    Y_list = []
    X_list = []

    for t in range(p, T):
        # stack lags [y_{t-1}, y_{t-2}, ..., y_{t-p}]
        lags = [Y_all[t - lag] for lag in range(1, p+1)]
        x_t = np.hstack(lags)  # shape (K*p,)
        if add_const:
            x_t = np.hstack([1.0, x_t])  # prepend intercept
        X_list.append(x_t)

        Y_list.append(Y_all[t])  # shape (K,)

    Y_mat = np.vstack(Y_list)        # (T-p, K)
    X_mat = np.vstack(X_list)        # (T-p, K*p+1) if const else (T-p, K*p)

    # also build last_lags for one-step-ahead forecast
    recent_lags = [Y_all[-lag] for lag in range(1, p+1)]
    x_last = np.hstack(recent_lags)
    if add_const:
        x_last = np.hstack([1.0, x_last])

    return Y_mat, X_mat, x_last

############################
# 3. Posterior update (Matrix-Normal / Inverse-Wishart)
############################

def bvar_posterior(Y_mat, X_mat, p, lambda_1=0.3, lambda_3=1.0):
    """
    Minnesota prior consistent with X_mat ordering from build_var_dataset:
      [y_{t-1,1},...,y_{t-1,K}, y_{t-2,1},...,y_{t-p,K}]
    """
    N, K = Y_mat.shape
    M = X_mat.shape[1]

    # --- Step 1: estimate AR(1) residual variances ---
    sigma_sq = np.zeros(K)
    for i in range(K):
        y_i = Y_mat[1:, i]
        x_i = Y_mat[:-1, i]
        beta_i = np.linalg.lstsq(x_i[:, None], y_i, rcond=None)[0]
        resid_i = y_i - x_i * beta_i
        sigma_sq[i] = np.var(resid_i, ddof=1)

    # --- Step 2: prior mean ---
    B0 = np.zeros((M, K))
    for i in range(K):
        # own first lag coefficient = 1
        idx = 1 + i  # intercept = 0, then first lag block starts at 1
        B0[idx, i] = 1.0

    # --- Step 3: prior variance matrix ---
    V0 = np.zeros((M, M))
    V0[0, 0] = 10.0  # intercept prior variance (loose)

    # fill lag coefficients
    for lag in range(1, p + 1):
        for j in range(K):  # regressor variable
            idx = 1 + (lag - 1) * K + j  # linear index for (lag, variable)
            for i in range(K):  # equation (dependent variable)
                if i == j:
                    var_ij = (lambda_1 ** 2) / (lag ** 2)
                else:
                    var_ij = (lambda_1 ** 2) * (sigma_sq[i] / sigma_sq[j]) / ((lambda_3 ** 2) * (lag ** 2))
                # same variance across equations for coefficient on this regressor
                V0[idx, idx] = var_ij

    # --- Step 4: prior on Sigma ---
    S0 = np.eye(K)
    nu0 = K + 2

    # --- Step 5: posterior update ---
    V0_inv = inv(V0)
    XtX = X_mat.T @ X_mat
    XtY = X_mat.T @ Y_mat
    Vn = inv(V0_inv + XtX)
    Bn = Vn @ (V0_inv @ B0 + XtY)

    resid = Y_mat - X_mat @ Bn
    Sn = S0 + resid.T @ resid
    nun = nu0 + N

    return Bn, Vn, Sn, nun

############################
# 4. Sampling from the posterior
############################

def sample_bvar_posterior(Bn, Vn, Sn, nun, draws=1000):
    """
    Draws (B, Sigma) pairs from the posterior:
      Sigma ~ InvWishart(Sn, nun)
      B | Sigma ~ MatrixNormal(Bn, Vn, Sigma)
    Returns:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
    """
    M, K = Bn.shape
    B_samps = np.zeros((draws, M, K))
    Sigma_samps = np.zeros((draws, K, K))

    for i in range(draws):
        # sample Sigma ~ InvWishart
        Sigma_i = invwishart.rvs(df=nun, scale=Sn)  # (K, K)
        Sigma_samps[i] = Sigma_i

        # sample B | Sigma
        # vec(B) ~ N(vec(Bn), kron(Sigma, Vn))
        # We can draw using multivariate_normal on the vectorized form:
        cov = np.kron(Sigma_i, Vn)  # (M*K, M*K)
        mean = Bn.flatten(order='F')  # vec in column-major (match kron convention)
        B_vec_i = multivariate_normal.rvs(mean=mean, cov=cov)
        B_i = B_vec_i.reshape(M, K, order='F')
        B_samps[i] = B_i

    return B_samps, Sigma_samps

############################
# 5. One-step-ahead predictive draws
############################

def forecast_one_step(B_samps, Sigma_samps, x_last):
    """
    Produce predictive draws for y_{T+1} given last lags x_last.
    Inputs:
      B_samps: (draws, M, K)
      Sigma_samps: (draws, K, K)
      x_last: shape (M,) = [1, last_lags...] if intercept was used
    Returns:
      y_pred_samps: (draws, K)
    """
    draws, M, K = B_samps.shape
    y_pred_samps = np.zeros((draws, K))
    for i in range(draws):
        B_i = B_samps[i]           # (M, K)
        Sigma_i = Sigma_samps[i]   # (K, K)

        mean_i = x_last @ B_i      # (K,)
        shock_i = multivariate_normal.rvs(mean=np.zeros(K), cov=Sigma_i)
        y_pred_samps[i] = mean_i + shock_i
    return y_pred_samps

############################
# 6. Put it all together
############################

p = 2  # VAR lag order
Y_mat, X_mat, x_last = build_var_dataset(Y_all, p, add_const=True)

# Get posterior hyperparameters
Bn, Vn, Sn, nun = bvar_posterior(Y_mat, X_mat, p, lambda_1=0.4, lambda_3=1.0)

# Draw from posterior
B_samps, Sigma_samps = sample_bvar_posterior(Bn, Vn, Sn, nun, draws=2000)

# Posterior predictive for next period
y_pred_samps = forecast_one_step(B_samps, Sigma_samps, x_last)
# y_pred_samps shape: (2000, K)

# Focus on the FIRST variable (column 0 = tn_air_traffic_value_short)
first_var_preds = y_pred_samps[:, 0]

print("Forecast mean for first variable:", first_var_preds.mean())
print("Forecast std for first variable:", first_var_preds.std())
print("95% credible interval for first variable:",
      np.percentile(first_var_preds, [2.5, 97.5]))

# --- Posterior predictive summary ---
means = y_pred_samps.mean(axis=0)
stds = y_pred_samps.std(axis=0)
ci_lower = np.percentile(y_pred_samps, 2.5, axis=0)
ci_upper = np.percentile(y_pred_samps, 97.5, axis=0)

summary_df = pd.DataFrame({
    "Variable": endog_cols,
    "Mean Forecast": means,
    "Std": stds,
    "95% Lower": ci_lower,
    "95% Upper": ci_upper
})
print("\n=== Posterior Forecast Summary ===")
print(summary_df)

# Visualization for this iteration
# visualization

# --- Plot forecast distributions ---
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    ax = axes[i]
    ax.hist(y_pred_samps[:, i], bins=30, color="skyblue", edgecolor="k", alpha=0.7)
    ax.axvline(means[i], color="red", lw=2, label="Mean forecast")
    ax.axvline(ci_lower[i], color="gray", ls="--")
    ax.axvline(ci_upper[i], color="gray", ls="--")
    ax.set_title(f"Posterior Predictive for {var}")
    ax.legend()
plt.tight_layout()
plt.show()

# --- Fitted vs Actual values (in-sample fit) ---
Y_fit = X_mat @ Bn
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    axes[i].plot(Y_mat[:, i], label="Actual", lw=2)
    axes[i].plot(Y_fit[:, i], label="Fitted", lw=2, linestyle="--")
    axes[i].set_title(f"In-sample Fit: {var}")
    axes[i].legend()
plt.tight_layout()
plt.show()

############################
# 8. Impulse Response Function (IRF) — basic version
############################

def impulse_response(B_samps, Sigma_samps, p, steps=10):
    draws, M, K = B_samps.shape
    # Take posterior mean parameters
    B_mean = B_samps.mean(axis=0)
    Sigma_mean = Sigma_samps.mean(axis=0)
    # reshape B to companion form
    A = B_mean[1:, :].T  # remove intercept, shape (K, K*p)
    comp = np.zeros((K*p, K*p))
    comp[:K, :] = A
    comp[K:, :-K] = np.eye(K*(p-1))
    # IRFs
    irfs = np.zeros((steps, K, K))
    irfs[0] = np.eye(K)
    for h in range(1, steps):
        irfs[h] = comp[:K, :K] @ irfs[h-1]
    # scale by Cholesky of Sigma
    P = np.linalg.cholesky(Sigma_mean)
    irfs = np.einsum("hij,jk->hik", irfs, P)
    return irfs

irfs = impulse_response(B_samps, Sigma_samps, p, steps=12)

# Plot IRFs
K = len(endog_cols)
fig, axes = plt.subplots(K, K, figsize=(3*K, 3*K))
for i in range(K):
    for j in range(K):
        axes[i,j].plot(irfs[:, i, j])
        axes[i,j].axhline(0, color="black", lw=1)
        axes[i,j].set_title(f"Response of {endog_cols[i]} to shock in {endog_cols[j]}")
plt.tight_layout()
plt.show()

# updated version with the MAE/RMSE components
# MINNESOTA version


# modified iteration of code (Akilan) for dealing with the two datasets

import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from numpy.linalg import inv
from scipy.stats import invwishart, multivariate_normal
from sklearn.metrics import mean_absolute_error, mean_squared_error

############################
# 1. Load and prep the data
############################

df = pd.read_csv("dataset_goods.csv") # Keep numeric endogenous variables (drop date) # Assumes all columns after 'date' are endogenous series
endog_cols = df.columns[1:] # skip 'date'
Y_all = df[endog_cols].to_numpy() # shape (T, K)

############################
# 2. Build VAR dataset
############################

def build_var_dataset(Y_all, p, add_const=True):
    T, K = Y_all.shape
    Y_list, X_list = [], []

    for t in range(p, T):
        lags = [Y_all[t - lag] for lag in range(1, p + 1)]
        x_t = np.hstack(lags)
        if add_const:
            x_t = np.hstack([1.0, x_t])
        X_list.append(x_t)
        Y_list.append(Y_all[t])

    Y_mat = np.vstack(Y_list)
    X_mat = np.vstack(X_list)

    recent_lags = [Y_all[-lag] for lag in range(1, p + 1)]
    x_last = np.hstack(recent_lags)
    if add_const:
        x_last = np.hstack([1.0, x_last])

    return Y_mat, X_mat, x_last

############################
# 3. Minnesota Prior setup
############################

def minnesota_prior(K, p, lambda_1=0.3, lambda_3=1.0, include_const=True):
    M = K * p + (1 if include_const else 0)
    B0 = np.zeros((M, K))
    V0 = np.zeros((M, M))

    for k in range(K):  # equation
        for j in range(K):  # variable
            for l in range(1, p + 1):  # lag
                idx = (1 if include_const else 0) + j * p + (l - 1)
                if j == k:
                    V0[idx, idx] = (lambda_1 / l) ** 2
                else:
                    V0[idx, idx] = ((lambda_1 * lambda_3) / l) ** 2

    if include_const:
        V0[0, 0] = 10.0  # intercept prior variance
    return B0, V0

############################
# 4. BVAR posterior update
############################

def bvar_posterior(Y_mat, X_mat, B0, V0, S0=None, nu0=None):
    N, K = Y_mat.shape
    M = X_mat.shape[1]
    V0_inv = inv(V0)
    XtX = X_mat.T @ X_mat
    XtY = X_mat.T @ Y_mat

    if S0 is None:
        S0 = np.eye(K)
    if nu0 is None:
        nu0 = K + 2

    Vn = inv(V0_inv + XtX)
    Bn = Vn @ (V0_inv @ B0 + XtY)
    resid = Y_mat - X_mat @ Bn
    term1 = resid.T @ resid
    term2 = (Bn - B0).T @ V0_inv @ (Bn - B0)
    Sn = S0 + term1 + term2
    nun = nu0 + N
    return Bn, Vn, Sn, nun

############################
# 5. Forecast
############################

def forecast_one_step(B_samps, Sigma_samps, x_last):
    draws, M, K = B_samps.shape
    y_pred_samps = np.zeros((draws, K))
    for i in range(draws):
        mean_i = x_last @ B_samps[i]
        shock_i = multivariate_normal.rvs(mean=np.zeros(K), cov=Sigma_samps[i])
        y_pred_samps[i] = mean_i + shock_i
    return y_pred_samps
############################
# 6. Evaluation loop
############################

p = 2
train_ratio = 0.8
T = Y_all.shape[0]
train_T = int(T * train_ratio)
K = Y_all.shape[1]

Y_train, Y_test = Y_all[:train_T], Y_all[train_T:]
Y_mat, X_mat, _ = build_var_dataset(Y_train, p, add_const=True)
B0, V0 = minnesota_prior(K, p, lambda_1=0.4, lambda_3=1.0)
Bn, Vn, Sn, nun = bvar_posterior(Y_mat, X_mat, B0, V0)

# Posterior draws
n_draws = 500
B_samps = np.zeros((n_draws, Bn.shape[0], K))
Sigma_samps = np.zeros((n_draws, K, K))
for i in range(n_draws):
    Sigma_i = invwishart.rvs(df=nun, scale=Sn)
    cov = np.kron(Sigma_i, Vn)
    mean = Bn.flatten(order='F')
    B_vec_i = multivariate_normal.rvs(mean=mean, cov=cov)
    B_samps[i] = B_vec_i.reshape(Bn.shape, order='F')
    Sigma_samps[i] = Sigma_i

# Recursive forecast and evaluation
preds, actuals = [], []
Y_hist = Y_train.copy()
for t in range(Y_test.shape[0]):
    _, _, x_last = build_var_dataset(Y_hist, p, add_const=True)
    y_pred_samps = forecast_one_step(B_samps, Sigma_samps, x_last)
    y_mean = y_pred_samps.mean(axis=0)
    preds.append(y_mean)
    actuals.append(Y_test[t])
    Y_hist = np.vstack([Y_hist, Y_test[t]])

preds = np.array(preds)
actuals = np.array(actuals)

############################
# 7. Compute MAE and RMSE
############################

mae = mean_absolute_error(actuals, preds)
rmse = np.sqrt(mean_squared_error(actuals, preds))

print(f"Overall MAE: {mae:.3f}")
print(f"Overall RMSE: {rmse:.3f}")

# Variable-level metrics
for i, col in enumerate(endog_cols):
    mae_i = mean_absolute_error(actuals[:, i], preds[:, i])
    rmse_i = np.sqrt(mean_squared_error(actuals[:, i], preds[:, i]))
    print(f"{col}: MAE = {mae_i:.3f}, RMSE = {rmse_i:.3f}")

# --- Posterior predictive summary ---
means = y_pred_samps.mean(axis=0)
stds = y_pred_samps.std(axis=0)
ci_lower = np.percentile(y_pred_samps, 2.5, axis=0)
ci_upper = np.percentile(y_pred_samps, 97.5, axis=0)

summary_df = pd.DataFrame({
    "Variable": endog_cols,
    "Mean Forecast": means,
    "Std": stds,
    "95% Lower": ci_lower,
    "95% Upper": ci_upper
})
print("\n=== Posterior Forecast Summary ===")
print(summary_df)

# visualization

# --- Plot forecast distributions ---
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    ax = axes[i]
    ax.hist(y_pred_samps[:, i], bins=30, color="skyblue", edgecolor="k", alpha=0.7)
    ax.axvline(means[i], color="red", lw=2, label="Mean forecast")
    ax.axvline(ci_lower[i], color="gray", ls="--")
    ax.axvline(ci_upper[i], color="gray", ls="--")
    ax.set_title(f"Posterior Predictive for {var}")
    ax.legend()
plt.tight_layout()
plt.show()

# --- Fitted vs Actual values (in-sample fit) ---
Y_fit = X_mat @ Bn
fig, axes = plt.subplots(len(endog_cols), 1, figsize=(8, 3*len(endog_cols)))
if len(endog_cols) == 1:
    axes = [axes]
for i, var in enumerate(endog_cols):
    axes[i].plot(Y_mat[:, i], label="Actual", lw=2)
    axes[i].plot(Y_fit[:, i], label="Fitted", lw=2, linestyle="--")
    axes[i].set_title(f"In-sample Fit: {var}")
    axes[i].legend()
plt.tight_layout()
plt.show()

############################
# 8. Impulse Response Function (IRF) — basic version
############################

def impulse_response(B_samps, Sigma_samps, p, steps=10):
    draws, M, K = B_samps.shape
    # Take posterior mean parameters
    B_mean = B_samps.mean(axis=0)
    Sigma_mean = Sigma_samps.mean(axis=0)
    # reshape B to companion form
    A = B_mean[1:, :].T  # remove intercept, shape (K, K*p)
    comp = np.zeros((K*p, K*p))
    comp[:K, :] = A
    comp[K:, :-K] = np.eye(K*(p-1))
    # IRFs
    irfs = np.zeros((steps, K, K))
    irfs[0] = np.eye(K)
    for h in range(1, steps):
        irfs[h] = comp[:K, :K] @ irfs[h-1]
    # scale by Cholesky of Sigma
    P = np.linalg.cholesky(Sigma_mean)
    irfs = np.einsum("hij,jk->hik", irfs, P)
    return irfs

irfs = impulse_response(B_samps, Sigma_samps, p, steps=12)

# Plot IRFs
K = len(endog_cols)
fig, axes = plt.subplots(K, K, figsize=(3*K, 3*K))
for i in range(K):
    for j in range(K):
        axes[i,j].plot(irfs[:, i, j])
        axes[i,j].axhline(0, color="black", lw=1)
        axes[i,j].set_title(f"Response of {endog_cols[i]} to shock in {endog_cols[j]}")
plt.tight_layout()
plt.show()

# Root Mean Squared Error Code and MAE
import numpy as np
from sklearn.metrics import mean_squared_error
list1 = []
list2 = []
list3 = []
target1 = []
target2 = []
target3 = []

print(np.mean(list1), np.mean(list2), np.mean(list3))
print(np.std(list1), np.std(list2), np.std(list3))

rmse1 = np.sqrt(mean_squared_error(list1, target1))
rmse2 = np.sqrt(mean_squared_error(list2, target2))
rmse3 = np.sqrt(mean_squared_error(list3, target3))
print(rmse1, rmse2, rmse3)